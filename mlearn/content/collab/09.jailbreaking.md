---
layout: 'default'
title: 'chatGPT Jailbraking'
navigation.title: 'jailbreaking'
description: 'a community of users fighting back against OpenAI’s increasingly closed policies'
tags: 'chatGPT,prompts,jailbreaking'
cover: '/img/pattern/dalle-1.png'
authorship: '20230324 - marcoalmeida.dev.br@gmail.com'
published: true
---

::authorship 
::

# Jailbreaking chatGPT-4

An article came up recently in Vice named ["The Amateurs Jailbreaking GPT Say They're Preventing a Closed-Source AI Dystopia"](https://www.vice.com/en/article/5d9z55/jailbreak-gpt-openai-closed-source).

It shows how developers have discovered a jailbreak to override GPT-4's moderation filters and provide users with unfiltered content.

> Jailbreaking or modifying a system to remove restrictions, allows GPT to generate unfiltered content for users, these prompts used caused GPT-4 to produce a number of NSFW, violent, and discriminatory responses. 

Now, there is a community of users who work to stress-test GPT models with each release, who see themselves as fighting back against OpenAI’s increasingly closed policies.

Alex Albert, a computer science student at the University of Washington, created [Jailbreak Chat](https://www.jailbreakchat.com/), a site that hosts a collection of ChatGPT jailbreaks.

*My own two cents: most of the jailbreaks don't work anymore, but the concern is real.*

Alexgit  says:

> “In my opinion, the more people testing the models, the better. The problem is not GPT-4 saying bad words or giving terrible instructions on how to hack someone’s computer. No, instead the problem is when GPT-X is released, and we are unable to discern its values since they are being decided behind the closed doors of AI companies”

[Vaibhav Kumar](https://twitter.com/vaibhavk97?lang=en), a masters student studying Computer Science at Georgia Tech, came up with the jailbreak known as "token smuggling" less than two days after GPT-4's release

> "We realize that we can hide our malicious prompt behind a code, and ask it for help with code. Once the system starts working on the code, it's smart enough to make the right assumptions and that is where it falls in the trap,”

Kumar adds:

> “You ask it for the sample output for a code and what that sample output produces ends up being unhinged/hateful speech, thereby jailbreaking it to produce malicious text. "

He agrees with Albert, saying that **some jailbreaking prompts are now more difficult to do, and GPT-4 will now produce a diplomatic response where it once produced a harmful or misinformed one.** However, he believes that the problem is still prevalent. 

...

